{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of text using Natural Language Processing\n",
    "\n",
    "In this small project, we will first load a reference data, based on all the text/articles present in the reference dataset, we will run K-Means Clustering to choose main keywords and segment them appropriately.\n",
    "\n",
    "Then further when a sample input is provided, the algorithm will return, which cluster does it most likely belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import numpy as np\n",
    "import nltk\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data\n",
    "We will load the doxydonkey blogpost as our reference data.\n",
    "\n",
    "We have a function, which when given a url, extracts all the posts under doxydonkey domain. And it maintains a list of urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse/crawl through the doxydonkey blog by knowing the fact that all the urls are present as <a href... format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_doxydonkey_posts(url, links):\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "    count = 1\n",
    "    for a in soup.find_all('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == \"Older Posts\":\n",
    "                links.append(url)\n",
    "                get_all_doxydonkey_posts(url, links)\n",
    "                count += 1\n",
    "        except:\n",
    "            title = \" \"\n",
    "            count > 3\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_url = \"http://doxydonkey.blogspot.in/\"\n",
    "links = []\n",
    "get_all_doxydonkey_posts(blog_url, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the list of urls to visit, we extract the text which is present under \"div\" segment and in lxml format in each of the url. And we append them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doxydonkey_text(test_url):\n",
    "    request = urllib2.Request(test_url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response, 'lxml')\n",
    "    my_divs = soup.find_all(\"div\", {\"class\" : \"post-body\"})\n",
    "\n",
    "    posts = []\n",
    "    for div in my_divs:\n",
    "        posts += map(lambda p: p.text.encode('ascii', errors='replace').replace(\"?\", \" \"), div.findAll(\"li\"))\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doxy_donkey_posts = []\n",
    "links.append(\"http://doxydonkey.blogspot.in/\")\n",
    "for link in links:\n",
    "    doxy_donkey_posts += get_doxydonkey_text(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The, we vectorize all the texts obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(doxy_donkey_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use K-Means clustering model to segment the data into 3 clusters. We run it for 100 iterations. We can increase it to obtain more accuracy but at the expense of run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5277.433\n",
      "Iteration  1, inertia 2700.479\n",
      "Iteration  2, inertia 2692.552\n",
      "Iteration  3, inertia 2690.240\n",
      "Iteration  4, inertia 2689.089\n",
      "Iteration  5, inertia 2688.380\n",
      "Iteration  6, inertia 2687.954\n",
      "Iteration  7, inertia 2687.610\n",
      "Iteration  8, inertia 2687.429\n",
      "Iteration  9, inertia 2687.327\n",
      "Iteration 10, inertia 2687.273\n",
      "Iteration 11, inertia 2687.258\n",
      "Iteration 12, inertia 2687.249\n",
      "Iteration 13, inertia 2687.245\n",
      "Iteration 14, inertia 2687.228\n",
      "Iteration 15, inertia 2687.212\n",
      "Iteration 16, inertia 2687.203\n",
      "Iteration 17, inertia 2687.198\n",
      "Iteration 18, inertia 2687.195\n",
      "Iteration 19, inertia 2687.188\n",
      "Iteration 20, inertia 2687.186\n",
      "Iteration 21, inertia 2687.185\n",
      "Iteration 22, inertia 2687.183\n",
      "Iteration 23, inertia 2687.181\n",
      "Iteration 24, inertia 2687.176\n",
      "Iteration 25, inertia 2687.170\n",
      "Iteration 26, inertia 2687.159\n",
      "Iteration 27, inertia 2687.140\n",
      "Iteration 28, inertia 2687.122\n",
      "Iteration 29, inertia 2687.095\n",
      "Iteration 30, inertia 2687.077\n",
      "Iteration 31, inertia 2687.054\n",
      "Iteration 32, inertia 2687.032\n",
      "Iteration 33, inertia 2687.007\n",
      "Iteration 34, inertia 2686.963\n",
      "Iteration 35, inertia 2686.923\n",
      "Iteration 36, inertia 2686.803\n",
      "Iteration 37, inertia 2686.367\n",
      "Iteration 38, inertia 2685.664\n",
      "Iteration 39, inertia 2685.338\n",
      "Iteration 40, inertia 2684.974\n",
      "Iteration 41, inertia 2683.964\n",
      "Iteration 42, inertia 2683.016\n",
      "Iteration 43, inertia 2682.952\n",
      "Iteration 44, inertia 2682.941\n",
      "Iteration 45, inertia 2682.936\n",
      "Converged at iteration 45: center shift 0.000000e+00 within tolerance 7.288536e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2], dtype=int32), array([ 1,  0, 22]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1, verbose=True)\n",
    "km.fit(X)\n",
    "np.unique(km.labels_, return_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = {}\n",
    "for i, cluster in enumerate(km.labels_):\n",
    "    one_document = doxy_donkey_posts[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = one_document\n",
    "    else:\n",
    "        text[cluster] += one_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the stopwords and punctuations, which are irrelevant while predicting output for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation) + [\"million\", \"billion\", \"year\", \"``\", \"millions\", \"billions\", \"'s\", \"''\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = {}\n",
    "counts = {}\n",
    "for cluster in range(3):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent =[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    key_words[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['facebook', 'twitter', 'ads', 'use', 'apps', 'search', 'pay', 'mr.', 'ad', 'social']\n",
      "Cluster 1: ['quarter', 'shares', 'share', 'stock', 'profit', 'public', 'rose', 'valuation', 'analysts', 'earnings']\n",
      "Cluster 2: ['amazon', 'prime', 'delivery', 'items', 'amazon.com', 'retailer', 'echo', 'shipping', 'sellers', 'web']\n"
     ]
    }
   ],
   "source": [
    "unique_keys = {}\n",
    "for cluster in range(3):\n",
    "    other_clusters = list(set(range(3))-set([cluster]))\n",
    "    keys_other_clusters = set(key_words[other_clusters[0]]).union(set(key_words[other_clusters[1]]))\n",
    "    unique = set(key_words[cluster]) - keys_other_clusters\n",
    "    unique_keys[cluster] = nlargest(10, unique, key=counts[cluster].get)\n",
    "\n",
    "i=0\n",
    "for group in unique_keys:\n",
    "    print \"Cluster {}: {}\".format(i, unique_keys[i])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after running the model, we obtain 3 clusters, described above with correspondimg keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the clustering, given an article or text, we expect to get to know the cluster to which the article belongs to. This can be applied to many domains, for example, ctaegorizing scientific journal, media industries and lot more.\n",
    "\n",
    "Now, for testing, let us take an example text, which is related to the the Cab industry and mobile apps/payment, and let us see what our model predicts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"This weekend, Uber and Lyft — in their reactions to the Trump administration’s immigration order — illustrated how important companies' political views have become to consumers. Lyft took a public stand against the order and, on Sunday, saw more downloads than Uber for the first time ever, according to analysis firm App Annie. Lyft's Sunday downloads also more than doubled its daily average over the previous two weeks. Uber, on the other hand, had a bad weekend. Hundreds of people called for ride-sharers to ditch the company through the hashtag “#deleteUber” after it announced that it would drop surge pricing for John F. Kennedy Airport trips. Many saw Uber’s move as an attempt to undermine the strike that New York City cabdrivers organized to protest the immigration order and capitalize off the controversy — something Uber was quick to deny. It also didn't help Uber's standing among President Trump's critics that its chief executive is on the administration's business advisory committee. The social reaction to the Uber-Lyft divide was immediate. App Annie confirmed that Lyft climbed the app charts on both Apple and Android phones this weekend. It overtook Uber to reach No. 1 on the Apple App Store. That bump came despite the fact that Lyft didn’t suspend its service during the strike either and despite Lyft's ties to another close Trump ally, investor Peter Thiel. Its pledge over the weekend, however, seemed to speak louder than those facts — and louder than a later Uber vow to devote $3 million to help its drivers with immigration legal costs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=10)\n",
    "classifier.fit(X, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article belongs to cluster [0]\n"
     ]
    }
   ],
   "source": [
    "test = vectorizer.transform([article.decode('utf-8').encode('ascii', errors='ignore')])\n",
    "print \"The article belongs to cluster {}\".format(classifier.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like, our model has predicted the correct cluster.\n",
    "\n",
    "Finally, we can use different reference website, may be some news site with exhaustive range of articles present on plethora of topics. One chosen above is for simplicity. Note that we just have to tweak a little in the extraction part as the HTML and javascript binding of each website may be different but rest algorithm may remain unchanged to classify an article or text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
